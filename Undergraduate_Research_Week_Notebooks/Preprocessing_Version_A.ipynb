{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing Data, Version A\n",
        "\n",
        "Using Large Language Models to add much needed context to interviews.\n",
        "\n",
        "This is a pre-processing document that prepares the Boder 2020 testimonies for pronoun disambiguation and context rewriting.\n",
        "\n",
        "This is following a similar format to the same clustering run in 2022 using SBert Clustering\n",
        "\n",
        "Authors: Billy Peir, off code from Michelle Lee\n"
      ],
      "metadata": {
        "id": "a8_jq8onYWRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import data"
      ],
      "metadata": {
        "id": "bRa8fS6gYyD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2lnj8FZg210",
        "outputId": "602ed42b-ac56-4435-81c9-c5985d64a928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#@title Import libraries\n",
        "import pandas as pd\n",
        "import glob\n",
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPbelmi_VtAz",
        "outputId": "88d11884-16df-4612-b215-4dea1d88020d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4qCxoWhg-Xc"
      },
      "outputs": [],
      "source": [
        "#@title Import data\n",
        "# load full dataframe of all testimony lines from Boder archive\n",
        "boder_df = pd.read_csv('/content/drive/MyDrive/Holocaust and Genocide Studies Digital Research Lab/SBERT Clustering Documentation-20220830T233256Z-001/SBERT Clustering Documentation/Data/Boder_transcripts_clean_manipulated.csv')\n",
        "\n",
        "# rename some columns\n",
        "boder_df.rename(columns={'id_new': 'file_num', 'words':'text'}, inplace=True)\n",
        "boder_df = boder_df.reset_index(drop=True)\n",
        "\n",
        "# create an index column as reference (used later for answer extraction)\n",
        "boder_df['manual_index'] = list(range(0, len(boder_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUnj4B7g-Xe"
      },
      "source": [
        "#Preprocessing Data\n",
        "\n",
        "Some additional preprocessing needs to be done for this corpus before we tokenize sentences. This includes removing lines with ellipses and certain bracketed comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnr8ySWwg-Xe"
      },
      "outputs": [],
      "source": [
        "# to clean up sentences for tokenizing,\n",
        "# remove ellipses from each line of text\n",
        "no_ellipses = []\n",
        "for i in range(len(boder_df)):\n",
        "    sentence = boder_df['text'].iloc[i]\n",
        "    # Check if sentence is a string before applying replace\n",
        "    if isinstance(sentence, str):\n",
        "        no_ellipses.append(sentence.replace('. . .', ''))\n",
        "    else:\n",
        "        # Handle non-string values (e.g., NaN)\n",
        "        no_ellipses.append(str(sentence))  # or any other desired handling\n",
        "boder_df['text'] = no_ellipses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPEODjTlg-Xe"
      },
      "outputs": [],
      "source": [
        "# remove bracketed statments except for 'unintelligble'\n",
        "bracketless_texts_list = []\n",
        "\n",
        "for i in range(len(boder_df)): # for each sentence string in the dataframe\n",
        "    string = boder_df.text.iloc[i]\n",
        "    for j in range(5):\n",
        "        substring_to_remove = string[string.find(\"[\")+1:string.find(\"]\")]\n",
        "        if substring_to_remove != 'unintelligible':\n",
        "            string = string.replace(\"[\"+substring_to_remove+\"]\", '')\n",
        "    as_list = boder_df.text.tolist()\n",
        "    idx = as_list.index(boder_df.text.iloc[i])\n",
        "    as_list[idx] = string\n",
        "    boder_df.text = as_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assemble Context\n",
        "\n",
        "Now, to prepare to prompt into OpenAI, assemble the context window neccessary to run the prompts"
      ],
      "metadata": {
        "id": "He4-NUDnZYnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set token limit and model\n",
        "token_limit = 500\n",
        "model = 'gpt-4o-mini'\n",
        "\n",
        "# Filter out NaN values in 'text' column\n",
        "boder_df_filtered = boder_df[boder_df['text'].notna()]"
      ],
      "metadata": {
        "id": "FQCBs2yTHULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Libraries\n",
        "!pip install tiktoken #allows for token estimation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "C5lYQHeDH_0D",
        "outputId": "6ee75a50-5b88-43a7-c3ea-db32b2d3d885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-10-4f786d061ea9>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-4f786d061ea9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install tiktoken #allows for token estimation\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create is_interviewee column\n",
        "boder_df_filtered['is_interviewee'] = [int(ele != \"David Boder\") for ele in boder_df_filtered['speaker']]\n",
        "boder_df_filtered.head()"
      ],
      "metadata": {
        "id": "8KGznkwRIKan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define a function to preprocess the dataset by assemblying the necessary context\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "def generate_context(df, model, token_limit):\n",
        "    \"\"\"\n",
        "    Generate context for each row with a rolling window of the previous context,\n",
        "    constrained by a token limit. The context will include text up to, but not including, the current row.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The DataFrame containing the text data.\n",
        "    model (str): The name of the LLM model for tokenization.\n",
        "    token_limit (int): The maximum number of tokens allowed for the context.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with an additional 'context' column.\n",
        "    \"\"\"\n",
        "    # Initialize the tokenizer based on the provided model\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "\n",
        "    contexts = []\n",
        "    current_context_lines = []  # Use a list to manage context lines\n",
        "    current_file = None\n",
        "    current_token_count = 0  # Keep track of current token count\n",
        "    tokens = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        # if 'is_interviewee' = 1, speaker = 'SUBJECT'\n",
        "        # if 'is_interviewee' = 0, speaker = 'INTERVIEWER'\n",
        "        # if 'is_interviewee' = 2, speaker = 'CREW'\n",
        "\n",
        "        if row['is_interviewee'] == 1:\n",
        "            speaker = 'SUBJECT'\n",
        "        elif row['is_interviewee'] == 0:\n",
        "            speaker = 'INTERVIEWER'\n",
        "        else:\n",
        "            speaker = 'CREW'\n",
        "\n",
        "        file_num = row['file_num']\n",
        "\n",
        "        # If it's a new interview file, reset the context\n",
        "        if current_file != file_num:\n",
        "            current_context_lines = []\n",
        "            current_token_count = 0\n",
        "            current_file = file_num\n",
        "\n",
        "        # Join the current context lines into a single string (before adding the current line)\n",
        "        context = ''.join(current_context_lines)\n",
        "        contexts.append(context)\n",
        "        tokens.append(current_token_count)\n",
        "\n",
        "        # Add the current line to the context AFTER the current context is saved\n",
        "        current_line = f\"{speaker}: {row['text']}\\n\"\n",
        "        current_line_tokens = encoding.encode(current_line)\n",
        "\n",
        "        # Add tokens of the new line to the total token count\n",
        "        current_token_count += len(current_line_tokens)\n",
        "        current_context_lines.append(current_line)\n",
        "\n",
        "        # Trim context if token count exceeds the limit\n",
        "        while current_token_count > token_limit:\n",
        "            # Remove the oldest line\n",
        "            if len(current_context_lines) > 1:\n",
        "              removed_line = current_context_lines.pop(0)\n",
        "              removed_line_tokens = encoding.encode(removed_line)\n",
        "              current_token_count -= len(removed_line_tokens)\n",
        "\n",
        "            else:\n",
        "              break\n",
        "    # Add the context to the dataframe\n",
        "    df['context'] = contexts\n",
        "    df['token_count'] = tokens\n",
        "    return df"
      ],
      "metadata": {
        "id": "lfF62MIuonY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_df = generate_context(boder_df_filtered.copy(), model, token_limit)"
      ],
      "metadata": {
        "id": "vN1CR_kNHgDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJdHC7L-g-Xd"
      },
      "source": [
        "# Extract Interviewer Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "l6i9bCbRg-Xd"
      },
      "outputs": [],
      "source": [
        "# for BERT clustering purposes, only keep rows with interview questions\n",
        "# Since Boder is the sole interviewer for this corpus,\n",
        "# we keep only the lines spoken by him\n",
        "boder_qs = context_df.loc[context_df['speaker'] == 'David Boder']\n",
        "\n",
        "len(boder_qs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boder_qs.head()"
      ],
      "metadata": {
        "id": "JD8Lg-10Jr62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide words column with multi-sentences into a list\n",
        "# using sentence tokenizer\n",
        "boder_df.text = boder_df.text.apply(lambda x: sent_tokenize(str(x)))"
      ],
      "metadata": {
        "id": "uk8MnSI86UHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wbd5OOUg-Xf"
      },
      "outputs": [],
      "source": [
        "# Now, keep only the questions/interrogative statements spoken by the interviewer.\n",
        "# we remove any sentence that doesn't include any of the key words/symbols\n",
        "# included in the list below.\n",
        "encoding = tiktoken.encoding_for_model(model)\n",
        "keys = ['?', 'tell', 'describe', 'share', 'sing', 'message', 'photograph', 'ask', 'did you', 'were you']\n",
        "\n",
        "id_list=[]\n",
        "sent_num_list=[]\n",
        "que_ans_list=[]\n",
        "manual_index_list=[]\n",
        "texts_list=[]\n",
        "context_list = []\n",
        "\n",
        "for i in range(len(boder_qs)):\n",
        "    current_context = boder_qs.context.iloc[i]\n",
        "    current_token_count = boder_qs.token_count.iloc[i]\n",
        "    for j in list(boder_qs.text.iloc[i]): # j is a sentence\n",
        "        if any(ele in j for ele in keys):\n",
        "            id_list.append(boder_qs.file_num.iloc[i])\n",
        "            sent_num_list.append(boder_qs.sent_num.iloc[i])\n",
        "            que_ans_list.append(boder_qs.que_ans.iloc[i])\n",
        "            manual_index_list.append(boder_qs.manual_index.iloc[i])\n",
        "            texts_list.append(j)\n",
        "            context_list.append(current_context)\n",
        "        current_line = \"INTERVIEWER: \" + j + \"\\n\"\n",
        "        current_context = ''.join(current_context) + current_line\n",
        "        current_token_count = current_token_count + len(encoding.encode(current_line))\n",
        "        # Trim context if token count exceeds the limit\n",
        "        while current_token_count > token_limit:\n",
        "            current_context = current_context.split('\\n')\n",
        "            # Remove the oldest line\n",
        "            if len(current_context) > 1:\n",
        "              removed_line = current_context[0]\n",
        "              current_context = '\\n'.join(current_context[1:])\n",
        "              removed_line_tokens = encoding.encode(removed_line)\n",
        "              current_token_count -= len(removed_line_tokens)\n",
        "\n",
        "            else:\n",
        "              break\n",
        "\n",
        "\n",
        "# append all questions and meta data into a data frame\n",
        "boder_intqs = pd.DataFrame()\n",
        "boder_intqs[\"file_num\"]=id_list\n",
        "boder_intqs[\"sent_num\"]=sent_num_list\n",
        "boder_intqs[\"manual_index\"]=manual_index_list\n",
        "boder_intqs[\"text\"]=texts_list\n",
        "boder_intqs[\"que_ans\"]=que_ans_list\n",
        "\n",
        "boder_intqs['context'] = context_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPxVqsdLg-Xf"
      },
      "outputs": [],
      "source": [
        "# omit lines which only contain 'unintelligible'\n",
        "boder_intqs = boder_intqs[boder_intqs['text'] != '[unintelligible]']\n",
        "boder_intqs = boder_intqs[boder_intqs['text'] != '[unintelligible].']\n",
        "boder_intqs = boder_intqs[boder_intqs['text'] != '[unintelligible] .']\n",
        "\n",
        "# omit lines which only contain '?' or '.'\n",
        "boder_intqs = boder_intqs[boder_intqs['text'] != '?']\n",
        "boder_intqs = boder_intqs[boder_intqs['text'] != '.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EqJWsSQtg-Xf"
      },
      "outputs": [],
      "source": [
        "boder_intqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBIb8QH0g-Xg"
      },
      "outputs": [],
      "source": [
        "#boder_intqs2.to_csv('Boder_questions_spe6.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq8Harttg-Xg"
      },
      "outputs": [],
      "source": [
        "# get question and answer word count and save as a column in the dataframe\n",
        "qword_count = []\n",
        "for index in qs_df.index: # Iterate using the index of qs_df\n",
        "    qword_count.append(len(str(qs_df.loc[index, 'text']).split()))\n",
        "\n",
        "qs_df['question_length'] = qword_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs_df"
      ],
      "metadata": {
        "id": "J1014bmt9PuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "break"
      ],
      "metadata": {
        "id": "_ageMj9VVJAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_df.to_csv('Version_A_Preprocessing_Results.csv')"
      ],
      "metadata": {
        "id": "ofXxFzRMiy9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}